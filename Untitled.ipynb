{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n",
    "\n",
    "from gameGrid import GameGrid\n",
    "import lightMaster\n",
    "import zombieMaster\n",
    "from env import Env\n",
    "from envManager import EnvManager\n",
    "from epsilonGreedyStrategy import EpsilonGreedyStrategy\n",
    "from zombieMaster import ZombieMaster\n",
    "from lightMaster import LightMaster\n",
    "from replayMemory import ReplayMemory\n",
    "from DQN import DQN\n",
    "from experience import Experience\n",
    "from qValues import QValues\n",
    "\n",
    "\"\"\"\n",
    "# create env_manager\n",
    "# create the two agents with initial params\n",
    "# play the game:\n",
    "    1. zombie master takes an action - places a zombie somewhere\n",
    "    2. light master takes an action - places the light somewhere\n",
    "    3. calculate rewards and let the agents learn from it\n",
    "    4. the environment is taking one step of all zombies\n",
    "\"\"\"\n",
    "# set seed\n",
    "np.random.seed(679)\n",
    "random.seed(679)\n",
    "\n",
    "# top parameters\n",
    "target_update = 10\n",
    "num_episodes = 1000\n",
    "# in env we defined 50 steps per episode\n",
    "\n",
    "# learning parameters\n",
    "batch_size = 256\n",
    "gamma = 0.999\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.001\n",
    "memory_size = 100000\n",
    "lr = 0.001\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython: from IPython import display\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = Env(GameGrid(8, 16))\n",
    "\n",
    "em = EnvManager(env, device)\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "\n",
    "agent_zombie = ZombieMaster(strategy, em.num_actions_available()[1], device)\n",
    "policy_net_zombie = DQN(em.get_screen_height(), em.get_screen_width(), env.action_space()[1]).to(device)\n",
    "target_net_zombie = DQN(em.get_screen_height(), em.get_screen_width(), env.action_space()[1]).to(device)\n",
    "memory_zombie = ReplayMemory(memory_size)\n",
    "target_net_zombie.load_state_dict(policy_net_zombie.state_dict())\n",
    "target_net_zombie.eval()\n",
    "optimizer_zombie = optim.Adam(params=policy_net_zombie.parameters(), lr=lr)\n",
    "\n",
    "agent_light = LightMaster(strategy, em.num_actions_available()[0], device)\n",
    "policy_net_light = DQN(em.get_screen_height(), em.get_screen_width(), env.action_space()[0]).to(device)\n",
    "target_net_light = DQN(em.get_screen_height(), em.get_screen_width(), env.action_space()[0]).to(device)\n",
    "memory_light = ReplayMemory(memory_size)\n",
    "target_net_light.load_state_dict(policy_net_light.state_dict())\n",
    "target_net_light.eval()\n",
    "optimizer_light = optim.Adam(params=policy_net_light.parameters(), lr=lr)\n",
    "\n",
    "episode_rewards = []\n",
    "\n",
    "\n",
    "def extract_tensors(experiences):\n",
    "    # Convert batch of Experiences to Experience of batches\n",
    "    batch = Experience(*zip(*experiences))\n",
    "\n",
    "    t1 = torch.cat(batch.state)\n",
    "    t2 = torch.cat(batch.action)\n",
    "    t3 = torch.cat(batch.reward)\n",
    "    t4 = torch.cat(batch.next_state)\n",
    "\n",
    "    return t1, t2, t3, t4\n",
    "\n",
    "\n",
    "def get_moving_average(period, values):\n",
    "    values = torch.tensor(values, dtype=torch.float)\n",
    "    if len(values) >= period:\n",
    "        moving_avg = values.unfold(dimension=0, size=period, step=1) \\\n",
    "            .mean(dim=1).flatten(start_dim=0)\n",
    "        moving_avg = torch.cat((torch.zeros(period - 1), moving_avg))\n",
    "        return moving_avg.numpy()\n",
    "    else:\n",
    "        moving_avg = torch.zeros(len(values))\n",
    "        return moving_avg.numpy()\n",
    "\n",
    "\n",
    "def plot(values, moving_avg_period):\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Zombies Survived')\n",
    "    plt.plot(values)\n",
    "\n",
    "    moving_avg = get_moving_average(moving_avg_period, values)\n",
    "    plt.plot(moving_avg)\n",
    "    plt.pause(0.001)\n",
    "    print(\"Episode\", len(values), \"\\n\",\n",
    "          moving_avg_period, \"episode moving avg:\", moving_avg[-1])\n",
    "    if is_ipython: display.clear_output(wait=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen = em.get_state()\n",
    "    \n",
    "plt.figure()\n",
    "plt.imshow(screen.squeeze(0).permute(1, 2, 0).cpu(), interpolation='none')\n",
    "plt.title('Starting state example')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = em.get_state()\n",
    "\n",
    "action_zombie = agent_zombie.select_action(state, policy_net_zombie)\n",
    "action_light = agent_light.select_action(state, policy_net_light)\n",
    "em.take_action(env.start_positions[action_zombie.numpy()], action_light.numpy())\n",
    "\n",
    "screen = em.get_state()\n",
    "screen.size()\n",
    "plt.figure()\n",
    "plt.imshow(screen.squeeze(0).permute(1, 2, 0).cpu(), interpolation='none')\n",
    "plt.title('Non starting state example')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen = em.get_state()\n",
    "screen.size()\n",
    "plt.figure()\n",
    "plt.imshow(screen.squeeze(0).permute(1, 2, 0).cpu(), interpolation='none')\n",
    "plt.title('Non starting state example')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
